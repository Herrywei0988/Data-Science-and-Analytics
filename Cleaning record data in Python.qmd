---
title: 'Lab-2.2: Cleaning record data in R and Python'
jupyter: python3
format:
     html:
          embed-resources: true
---


*Author James Hickman, post any questions to slack* 

**Instructions** 

* Part-1: Read and work through all tutorial content and do all exercises below using python
* Part-2: Create a .rmd file, copy all markdown content into the .rmd file, and repeat all coding exercises below in R instead of python (using tibbles instead of data-frames)

**Submission:**

* You need to upload TWO documents to Canvas when you are done
  * (1) A PDF (or HTML) of the completed .ipynb document (python submission) 
  * (2) A PDF (or HTML) of the completed .qmd document (R submission) 
* **BOTH ARE REQUIRED, YOU CANNOT DO JUST ONE OR THE OTHER**
* The final uploaded version should NOT have any code-errors present 
* All outputs must be visible in the uploaded version, including code-cell outputs, images, graphs, etc


### Cleaning record data 

In practice, there is no "one size fits all" method for data cleaning. 

How you clean the data depends on your objective and what kind of data you have (e.g. record, text, transaction,.. etc) 

Record data is very common, therefore we will focus on this as a example case, specifically the following common operations. 

* merging data frames
* Removing un-needed columns 
* Dealing with missing values and non-sense values 
* standardization 
* normalization 

We will intentionally focus on an overly simply toy data set, the focus is on the operations, not the complexity of the data

```{python}
import pandas as pd
import numpy as np
```

### Read in the data 

* Use pandas to read in the "example-1.csv" and "example-2.csv" files in the folder named "data"
  
* Before moving further, open the files in VSC, install relevant .csv extensions (optional), manually expect the data and look for obvious issues 

```{python}
# INSERT CODE TO READ THE DATA FILES (hint .. google --> "how to read csv file with pandas")
df1 = pd.read_csv('data/example-1.csv')
df2 = pd.read_csv('data/example-2.csv')
```

```{python}
print(df1)
```

```{python}
print(df2)
```

```{python}
# LOOK AT FIRST COUPLE LINES
print(df2.head(2))
```

```{python}
# LOOK AT LAST COUPLE LINES
print(df2.tail(2))
```

Notice that some column names have spaces, lets get rid of those

```{python}
## INSERT CODE TO REMOVE SPACES FROM COLUMN NAMES

# Remove leading and trailing spaces from column names in df1 and df2 (without replacing spaces in the middle)
df1.columns = df1.columns.str.strip()
df2.columns = df2.columns.str.strip()

# Display the updated column names to confirm
print("Updated columns in df1:", df1.columns)
print("Updated columns in df2:", df2.columns)
```

Lets also clean up some of the column names

```{python}
# INSERT CODE TO RENAME THE COLUMN NAME "age" --> "age_years" and "country" as "country_of_origin"
# PRINT THE MODIFIED COLUMN NAMES WHEN DONE

# Now, rename 'age' in df1 to 'age_years' and 'country' in df2 to 'country_of_origin'
df1 = df1.rename(columns={'age': 'age_years'})
df2 = df2.rename(columns={'country': 'country_of_origin'})

# Display the modified column names to confirm the changes
print("Modified columns in df1:", df1.columns)
print("Modified columns in df2:", df2.columns)
```

### Standardizing values 

Standardization typically means forcing everything to use the same "standard" units 

For example, making sure all weights are in lb, all money values are in USD, etc. 

You can typically use any units, it doesn't really matter which you choose as long as everything is the same.

It is good to do this before merging your data, because it would be very bad to have a column of data with mixed units. 

A column with mixed units is basically non-sense. 

For example, what is the meaning of averaging a vector of house prices where half of the values are USD and the other half is Pesos.

```{python}
#INSERT CODE TO CONVERT TYPECAST "housing_payment_pesos" AS TYPE "FLOAT"

df2['housing_payment_pesos'] = df2['housing_payment_pesos'].astype(float)

# Display the data types of df2 to confirm the conversion
print(df2.dtypes)
```

```{python}
#INSERT CODE TO CONVERT "housing_payment_pesos" to USD
# 1 Mexican Peso = 0.050 USD
# 1 USD = 19.88 Mexican Peso

pesos_to_usd_rate = 0.050

# Convert the 'housing_payment_pesos' column to USD
df2['housing_payment_usd'] = df2['housing_payment_pesos'] * pesos_to_usd_rate

# Display the updated dataframe to check the conversion
print(df2[['housing_payment_pesos', 'housing_payment_usd']].head())
```

```{python}
#INSERT CODE TO RENAME "housing_payment_pesos" to "housing_payment_usd"

if 'housing_payment_usd' in df2.columns:
    df2 = df2.drop(columns=['housing_payment_usd'])

pesos_to_usd_rate = 0.050

df2['housing_payment_usd'] = df2['housing_payment_pesos'] * pesos_to_usd_rate

df2 = df2.drop(columns=['housing_payment_pesos'])
```

```{python}
# PRINT THE MODIFIED DATA FRAME
print(df2)
```

another part of the standardization process is making sure all labels are consistently defined  

Notice how "usa", "US", etc

```{python}
## RUN THE FOLLOWING CELL TO REMOVE ANY WHITE SPACE FROM "country_of_origin"
df2['country_of_origin']=df2['country_of_origin'].str.strip()
```

```{python}
# INSERT CODE TO MAKE SURE ALL "US" TAGS equal "usa" and all "MEX" tags equal "mex"


df2['country_of_origin'] = df2['country_of_origin'].str.strip('""').str.strip().str.lower()
df2['country_of_origin'] = df2['country_of_origin'].replace({'us': 'usa', 'mex': 'mex'})

print(df2['country_of_origin'].unique())
```

```{python}
# PRINT THE DATA FRAME
print(df2)
```

### Merging data sets 

The easiest way to merge data files is when a "common-key" exists (i.e. a column shared by both files) 

In our toy example the customer_id can be used as a common key 

Data sets are typically merged using SQL type join operations (inner,outer,left,right). 

See the lecture slides for examples and codes for these join operations. 

```{python}
# INSERT CODE TO DO AN "OUTER" JOIN FOR THE TWO DATA-FRAMES USING "CUSTOMER_ID" AS COMMON KEY
# (hint .. see lecture slides)

df = pd.merge(df1, df2, on='customer_id', how='outer')
```

```{python}
print(df)
```

Notice how the merge injected "NaN" where merge wasn't possible

Notice that we also have missing values (empty cells), let's replace those with NaN. 

That way all missing values will be represented the same way

```{python}
# INSERT CODE TO: REPLACE ALL CELLS THAT ARE ENTIRELY SPACE (OR EMPTY) WITH NAN 
# (use google to figure out how to do this)
df = df.replace(r'^\s*$', pd.NA, regex=True)
```

```{python}
print(df)
```

```{python}
# INSERT CODE TO PRINT THE SHAPE OF THE NEW DATAFRAME
print(df.shape)
```

Lets see how many missing values there are in each column

```{python}
# INSERT CODE TO COUNT THE NUMBER OF MISSING VALUES IN EACH COLUMN (use google)
print(df.isna().sum())
```

```{python}
# INSERT CODE TO PRINT THE COLUMN NAMES

print(df.columns)
```

### Throw away un-needed columns

You don't want to remove an otherwise "good" row, just because a value is missing in a column that you don't care about.

Therefore, typically it is good to get rid of un-needed columns before removing missing values. 

Lets assume that the variables "initials" and "num_children" won't be needed for future analysis and can be thrown away

```{python}
print(df)
```

```{python}
### INSERT CODE TO REMOVE THE COLUMNS "initials" AND "num_children", 
df = df.drop(columns=['initials', 'num_children'])
```

```{python}
# INSERT CODE TO PRINT THE NEW DATA-FRAME AND ITS SHAPE

print(df)
print(df.shape)
```

Before dealing with the missing values, notice that one row has "nan", which is a string, instead of the numpy NaN object.

Lets fix that. 

```{python}
## INSERT CODE TO REPLACE THE STRING "nan" WITH NAN, PRINT THE NEW DATA-FRAME WHEN DONE

df = df.replace("nan", np.nan)

print(df)
```

### Dealing with non-sense values 

Often there are values in the data that are clearly NOT legitimate, such as negative ages, or strings where numbers should be.

You want to remove these before doing any averaging or other statistics because they will tamper with the results. 

For example, the average is highly sensitive to outliers, so negative ages would badly skew the mean.

```{python}
## RUN THE FOLLOWING CODE, THIS USES A CONDITIONAL TO ONLY KEEP ROWS WHERE "age_years"=NaN 
df = df[df['age_years'].notna()]
```

```{python}
## INSERT CODE TO REPLACE ANY NEGATIVE "age_years" WITH NUMPY "NaN" OBJECT
## There are multiple ways to do this, for example you can iterate over the 
# rows and use apply with a lambda function to enforce the conditional

df['age_years'] = pd.to_numeric(df['age_years'], errors='coerce')

df['age_years'] = df['age_years'].apply(lambda x: np.nan if x < 0 else x)
```

```{python}
## RUN THE FOLLOWING CELL TO REMOVE ANY WHITE SPACE FROM "yearly_income_usd"
df['yearly_income_usd']=df['yearly_income_usd'].str.strip()
```

```{python}
## INSERT CODE TO REPLACE ANY "yearly_income_usd" THAT IS A STRING WITH NUMPY nan OBJECT

df['yearly_income_usd'] = pd.to_numeric(df['yearly_income_usd'], errors='coerce')
```

```{python}
# PRINT THE DATA FRAME
print(df)
```

### Dealing with missing values 

There are many options to deal with missing values, some are better than others

The easiest, and probably the worst option, is to just throw out any row with NaN

```{python}
# INSERT CODE TO THROW AWAY ANY ROW WITH "NaN" 
# JUST PRINT THE OUTPUT, DONT RE-DEFINE THE DATAFRAME
# hint: read the documentation https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html

print(df.dropna())
```

**IMPORTANT README**

Another option would be to replace the missing value with a typical value that summarizes that column, such as the mean, median, or mode. 

In practice, you need to be careful when doing this! You are essentially tampering with the data. 

You MUST document changes of this kind and be TRANSPARENT, otherwise it could be viewed as academic or professional dishonesty. 

Especially if it dramatically effects your results.

Doing so will also effect all future analysis, since you are forcing a data point to become an "average" data point. 

For example, if you were looking for good basketball player's in a data driven way. Then replacing an unknown height with the "mean" would likely be a bad idea. 

In this case, you are making someone who might be 7 ft tall appear to be average height. 

You could also use more sophisticated methods such as MICE or regression to fill in the missing values (more on this next week).

Finally, you might be able to logically infer the value from other columns in the data set.

For example, if you have data on the price of someones house, you could likely predict their monthly payment assuming a typical 10-20% down payment and using a mortgage payment calculation formula.

This would be acceptable as long as you document your assumptions, approximations, and methods. 

```{python}
# SOMETIME PANDAS READS COLUMNS IN AS STRINGS RATHER THAN NUMBERS
# INSERT CODE TO TYPE-CAST ALL OF THE FOLLOWING COLUMNS AS FLOATS
# ["customer_id","age_years","account_balance_usd","yearly_income_usd","housing_payment_usd"]

df['customer_id'] = pd.to_numeric(df['customer_id'], errors='coerce')
df['age_years'] = pd.to_numeric(df['age_years'], errors='coerce')
df['account_balance_usd'] = pd.to_numeric(df['account_balance_usd'], errors='coerce')
df['yearly_income_usd'] = pd.to_numeric(df['yearly_income_usd'], errors='coerce')
df['housing_payment_usd'] = pd.to_numeric(df['housing_payment_usd'], errors='coerce')

print(df.dtypes)
```

```{python}
# INSERT CODE TO COMPUTE AND PRINT THE MEAN,MEDIAN, AND STD DOWN THE COLUMNS (DO EACH IN ITS OWN CELL)
# NOTICE THAT ONLY THE NUMERICAL ROWS ARE COMPUTED (YOU CAN IGNORE ANY DEPRECATION WARNINGS)
# print((df[["age_years","account_balance_usd"]]).mean(axis=1))

#MEAN

print("Mean values:\n", df[['age_years', 'account_balance_usd', 'yearly_income_usd', 'housing_payment_usd']].mean())
```

```{python}
#MEDIAN
 
print("Median values:\n", df[['age_years', 'account_balance_usd', 'yearly_income_usd', 'housing_payment_usd']].median())
```

```{python}
#STD

print("Standard Deviation values:\n", df[['age_years', 'account_balance_usd', 'yearly_income_usd', 'housing_payment_usd']].std())
```

Lets now replace some of the "NaN" data with typical missing values 

```{python}
# INSERT CODE TO REPLACE ANY "NaN" in "age_years" WITH THE AVERAGE

age_mean = df['age_years'].mean()
df['age_years'].fillna(age_mean, inplace=True)
```

```{python}
# INSERT CODE TO REPLACE ANY "NaN" in "yearly_income_usd" WITH THE MEDIAN

income_median = df['yearly_income_usd'].median()
df['yearly_income_usd'].fillna(income_median, inplace=True)
```

```{python}
# print the dataframe
print(df)
```

Now lets just throw away any additional rows with remaining NaN values 

```{python}
# INSERT CODE TO THROW AWAY ANY ROW WITH "NaN" 
# THIS TIME RE-DEFINE THE DATAFRAME WITHOUT THE "NaN"
# hint: read the documentation https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html

df = df.dropna()
```

```{python}
#PRINT THE DATA FRAME
print(df)
```

## De-duplication

Notice how the customer_id=1 is represented twice in the data set. 

This can be un-desireable, since it adds extra weight (importance) to that data point. 

For example, linear-regression would put twice as much importance on fitting that point as opposed to the others.

```{python}
# INSERT CODE TO REMOVE ROWS WITH DUPLICATES IN "customer_id" (KEEP THE FIRST VALUE ENCOUNTERS)

df = df.drop_duplicates(subset='customer_id', keep='first')
```

```{python}
# PRINT THE DATAFRAME
print(df)
```

At this point we have relatively "clean" data

### Normalization 

Normalization of a vector is the process of making the vector have a unit length (i.e. $|\mathbf{v}|=1$)

$\mathbf{v}_{norm}=\frac{\mathbf{v}}{|\mathbf{v}|}$

We can do a similar thing with a vector of data (e.g. h=heights with units ft). 

This is done using the following normalization process 

$\mathbf{h}_{norm}=\frac{\mathbf{h}-\mu_h}{\sigma_h}$

Where $\mu_h, \sigma_h$ are the mean (center) and standard deviation (width) of the height distribution.

This process makes a new vector $\mathbf{h}_{norm}$ which is a dimensionless quanity, meaning that it doesn't have units. 

The units cancel in the division arising during the normalization equation. 

This also forces the data into a standard range of roughly [-3 to 3] while still preserving the "shape" of the data.

Often, when training machine learning models, it is important to normalize the data first. 

The model will have a much easier time "fitting" if every input is in a standard range of [-3 to 3]

You can always "un-do" the normalization and re-assign the units by algebraically re-arranging the formula.

$\mathbf{h}=\mathbf{h}_{norm} \sigma_h+\mu_h$


```{python}
# INSERT CODE TO NORMALIZE THE COLUMN "housing_payment_usd" 

mean_hp = df['housing_payment_usd'].mean()
std_hp = df['housing_payment_usd'].std()

df['housing_payment_normalized'] = (df['housing_payment_usd'] - mean_hp) / std_hp
```

```{python}
# INSERT CODE TO RENAME THE COLUMN "housing_payment_usd" --> "housing_payment_normalized"

df.rename(columns={'housing_payment_usd': 'housing_payment_normalized'}, inplace=True)
```

```{python}
# PRINT THE DATA FRAME
print(df)
```

Notice how the values for "housing_payment_normalized" are in the range -3 to 3, rather than 1000 to 2000$

